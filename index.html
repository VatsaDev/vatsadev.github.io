<!doctype html>
<html lang="en">
    <head>
        <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DKFJQDBVVF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-DKFJQDBVVF');
</script>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta http-equiv="X-UA-Compatible" content="ie=edge" />
        <title>Vatsa</title>
        <link rel="stylesheet" href="style.css" type="text/css" media="all" />
        <link
            rel="shortcut icon"
            href="images/Favicon.png"
            type="image/x-icon"
        />
        <script
            src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.2/p5.min.js"
            integrity="sha512-eu9vkh+EbAsW3fMmPTj/DP5W3UegIdu0Z/OABMocvoofx43MYBkcQ9hRIVxZndV1vcCYQwBg+U1PkWl04TD0Jg=="
            crossorigin="anonymous"
            referrerpolicy="no-referrer"
        ></script>
    </head>
    <body>
        <h1>Vatsadev</h1>
        <p>
            Working on Robotics/ML/CV,
            <a href="https://vatsadev.github.io/3d/">3D mode</a>
        </p>
        <h2>Articles</h2>
        <ul>
            <li><a href="articles/quants.html">Getting better models by dropping floating point sizes and increasing parameters</a></li>
            <li>
                <a href="articles/BAGS.html"
                    >The effects of Grad_Acc and Batch_Size</a
                >
            </li>
            <li>
                <a href="articles/Layers.html"
                    >Whats better: Neural nets wider with less layers or thinner with more layers</a
                >
            </li>
            <li>
                <a href="articles/monkethot.html"
                    >Monkey thot compression: musings on the neuralink challenge</a
                >
            </li>
            <li>
                <a href="articles/memleak.html"
                    >Hallucinations and Memory leaks: Transformers</a
                >
            </li>
            <li>
                <a href="articles/transformerMath.html"
                    >Transformers learn patterns, math is patterns</a
                >
            </li>
            <li>
                <a href="articles/Nanophi.html"
                    >NanoPhi: replicating parts of Phi-1.5</a
                >
            </li>
        </ul>
        <h2>Threads/Posts</h2>
        <ul>
            <li> FRC 2024 Crescendo
                <a
                    href="https://twitter.com/_VatsaDev_/status/1777490972567609542"
                    >(...)</a
                >
                <a
                    href="https://twitter.com/_VatsaDev_/status/1777495993652740472"
                    >(...)</a
                >
                <a
                    href="https://twitter.com/_VatsaDev_/status/1777496607468159160"
                    >(...)</a
                >
            </li>
            <li>
                Lilith <a
                    href="https://twitter.com/_VatsaDev_/status/1760346381443912072"
                    >(...)</a
                > <a
                    href="https://twitter.com/_VatsaDev_/status/1760315664664035774"
                    >(...)</a
                > <a
                    href="https://twitter.com/_VatsaDev_/status/1760180145401512274"
                    >(...)</a>
            </li>
            <li>
                Clip Similarity and Sora <a
                    href="https://twitter.com/_VatsaDev_/status/1764741761673605125"
                    >(...)</a
                > <a
                    href="https://twitter.com/_VatsaDev_/status/1764666680779977002"
                    >(...)</a
                >
            </li>
            <li>
                TinyGrad dependencies <a
                    href="https://twitter.com/_VatsaDev_/status/1727891633491046806"
                    >(...)</a
                >
            </li>
        </ul>
        <h2>Resources</h2>
        <ul>
            <li>
                <a href="articles/CSHScomp.html"
                    >CSHS: Notes on competitions we've been to</a
                >
            </li>
            <li>
                <a href="articles/neuron.html"
                    >How a neuron learns: Linear layers</a
                >
            </li>
            <li>
                <a href="articles/fdvr.html"
                    >Thoughts on FDVR</a
                >
            </li>
        </ul>
        <br />
        <a href="https://x.com/_vatsadev">X</a><br />
        <a href="https://github.com/VatsaDev">Github</a><br />
        <a href="https://huggingface.co/VatsaDev">HF(Models/Datasets)</a>
        <script src="sketch.js"></script>
    </body>
</html>
